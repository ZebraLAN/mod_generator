# -*- coding: utf-8 -*-
"""ç”Ÿæˆå­—ä½“é…ç½®æ–‡ä»¶

è‡ªåŠ¨æµ‹å®šå­—ä½“ metrics å¹¶ç”Ÿæˆ ui/font_config.pyã€‚
ä½¿ç”¨ç¼“å­˜æœºåˆ¶é¿å…é‡å¤æµ‹å®šæœªå˜åŒ–çš„å­—ä½“ã€‚

ä½¿ç”¨æ–¹æ³•:
    python codegen/generate_font_config.py

å·¥ä½œæµç¨‹:
    1. è¯»å–ä¸‹æ–¹ FONT_PATHS é…ç½®
    2. æ£€æŸ¥å­—ä½“æ–‡ä»¶ mtimeï¼Œå¯¹æ¯”ç¼“å­˜
    3. å¯¹å˜åŒ–çš„å­—ä½“é‡æ–°æµ‹å®š metrics
    4. è®¡ç®— baseline offset å’Œ icon scale
    5. ç”Ÿæˆ ui/font_config.py

ä¾èµ–:
    pip install fonttools
"""

from __future__ import annotations

import hashlib
import json
import os
import random
import sys
from datetime import datetime
from pathlib import Path

try:
    from fontTools.ttLib import TTFont
    from fontTools.pens.boundsPen import BoundsPen
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… fonttools")
    print("è¿è¡Œ: pip install fonttools")
    sys.exit(1)


# ==================== å­—ä½“è·¯å¾„é…ç½® ====================
# ä¿®æ”¹è¿™é‡Œæ¥åˆ‡æ¢å­—ä½“ï¼Œç„¶åè¿è¡Œæ­¤è„šæœ¬é‡æ–°ç”Ÿæˆé…ç½®

FONT_PATHS = {
    # è‹±æ–‡å­—ä½“ (ä¸»å­—ä½“ï¼Œå†³å®š baseline)
    "english": "fonts/english/PlaywriteGBS-Regular.ttf",

    # ä¸­æ–‡å­—ä½“ (åˆå¹¶åˆ°è‹±æ–‡å­—ä½“)
    "chinese": "fonts/chinese/WenYue_GuDianMingChaoTi_JRFC.otf",

    # å›¾æ ‡å­—ä½“ (FA å­é›†)
    "icon": "fonts/icons/fa-subset.ttf",
}

# ==================== å›¾æ ‡ç¼©æ”¾ (è®¾è®¡å‚æ•°) ====================
# å›¾æ ‡ç›¸å¯¹äºæ–‡å­—çš„è§†è§‰å¤§å°ï¼Œ1.0 = åŸå§‹å¤§å°
# è°ƒæ•´æ­¤å€¼åï¼Œglyph_offset ä¼šè‡ªåŠ¨é‡æ–°è®¡ç®—
ICON_SCALE = 1.0


# ==================== è¾“å‡ºé…ç½® ====================

OUTPUT_FILE = "ui/font_config.py"
CACHE_FILE = ".font_metrics_cache.json"
BASE_FONT_SIZE = 16.0  # æµ‹å®šæ—¶ä½¿ç”¨çš„åŸºå‡†å­—å·


# ==================== ç¼“å­˜ç®¡ç† ====================


def get_file_hash(path: str) -> str:
    """è®¡ç®—æ–‡ä»¶ MD5 hash (å‰ 64KB)"""
    try:
        with open(path, "rb") as f:
            # åªè¯»å–å‰ 64KBï¼Œè¶³å¤ŸåŒºåˆ†ä¸åŒå­—ä½“
            return hashlib.md5(f.read(65536)).hexdigest()
    except Exception:
        return ""


def load_cache() -> dict:
    """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {}


def save_cache(cache: dict) -> None:
    """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, indent=2, ensure_ascii=False)


def is_cache_valid(path: str, cache: dict) -> bool:
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
    if path not in cache:
        return False

    entry = cache[path]

    # æ£€æŸ¥ mtime
    try:
        current_mtime = os.path.getmtime(path)
        if abs(current_mtime - entry.get("mtime", 0)) > 1:
            return False
    except Exception:
        return False

    # æ£€æŸ¥ hash (å¯é€‰ï¼Œæ›´å¯é )
    if "hash" in entry:
        current_hash = get_file_hash(path)
        if current_hash != entry["hash"]:
            return False

    return True


# ==================== å­—ä½“åº¦é‡æµ‹å®š ====================


def get_font_metrics(font_path: str) -> dict | None:
    """è¯»å–å­—ä½“çš„å…³é”® metrics"""
    if not os.path.exists(font_path):
        return None

    try:
        font = TTFont(font_path)
    except Exception as e:
        print(f"  âŒ æ— æ³•è¯»å–å­—ä½“: {e}")
        return None

    head = font["head"]
    upm = head.unitsPerEm
    scale = BASE_FONT_SIZE / upm

    metrics = {
        "units_per_em": upm,
    }

    # hhea è¡¨ (ImGui ä½¿ç”¨)
    if "hhea" in font:
        hhea = font["hhea"]
        metrics["hhea_ascent"] = round(hhea.ascent * scale, 2)
        metrics["hhea_descent"] = round(hhea.descent * scale, 2)
        metrics["hhea_line_gap"] = round(hhea.lineGap * scale, 2)

    # OS/2 è¡¨ (è¡¥å……)
    if "OS/2" in font:
        os2 = font["OS/2"]
        metrics["typo_ascent"] = round(os2.sTypoAscender * scale, 2)
        metrics["typo_descent"] = round(os2.sTypoDescender * scale, 2)
        metrics["cap_height"] = round(getattr(os2, "sCapHeight", 0) * scale, 2)
        metrics["x_height"] = round(getattr(os2, "sxHeight", 0) * scale, 2)

    return metrics


def sample_glyph_heights(font_path: str, codepoints: list[int], sample_count: int = 0) -> dict | None:
    """é‡‡æ ·å­—å½¢çš„å®é™…æ¸²æŸ“é«˜åº¦

    Args:
        font_path: å­—ä½“æ–‡ä»¶è·¯å¾„
        codepoints: è¦é‡‡æ ·çš„ç ç‚¹åˆ—è¡¨
        sample_count: é‡‡æ ·æ•°é‡ï¼Œ0 = å…¨é‡é‡‡æ ·
    """
    if not os.path.exists(font_path):
        return None

    try:
        font = TTFont(font_path)
    except Exception:
        return None

    cmap = font.getBestCmap()
    if not cmap:
        return None

    upm = font["head"].unitsPerEm
    scale = BASE_FONT_SIZE / upm

    # ç­›é€‰å­˜åœ¨çš„ç ç‚¹
    valid_codepoints = [cp for cp in codepoints if cp in cmap]
    if not valid_codepoints:
        return None

    # éšæœºé‡‡æ · (å¦‚æœæŒ‡å®šäº†æ•°é‡ä¸”å°äºæ€»æ•°)
    if sample_count > 0 and len(valid_codepoints) > sample_count:
        valid_codepoints = random.sample(valid_codepoints, sample_count)

    heights = []
    centers = []

    for cp in valid_codepoints:
        glyph_name = cmap[cp]
        try:
            pen = BoundsPen(font.getGlyphSet())
            font.getGlyphSet()[glyph_name].draw(pen)
            if pen.bounds:
                x_min, y_min, x_max, y_max = pen.bounds
                height = (y_max - y_min) * scale
                center = ((y_min + y_max) / 2) * scale
                heights.append(height)
                centers.append(center)
        except Exception:
            pass

    if not heights:
        return None

    return {
        "sample_count": len(heights),
        "avg_height": round(sum(heights) / len(heights), 2),
        "avg_center": round(sum(centers) / len(centers), 2),
        "max_height": round(max(heights), 2),
    }


def get_gb2312_codepoints() -> list[int]:
    """è·å– GB2312 å­—ç¬¦é›†ç ç‚¹"""
    codepoints = []
    for area in range(16, 88):
        for position in range(1, 95):
            try:
                code = bytes([area + 0xA0, position + 0xA0])
                char = code.decode("gb2312")
                codepoints.append(ord(char))
            except Exception:
                continue
    return codepoints


def get_icon_codepoints() -> list[int]:
    """è·å–å›¾æ ‡å­—ä½“ç ç‚¹èŒƒå›´ (Font Awesome PUA)"""
    return list(range(0xE000, 0xF8FF + 1))


def get_english_codepoints() -> list[int]:
    """è·å–è‹±æ–‡å¯¹é½å‚ç…§å­—ç¬¦ç ç‚¹

    ä½¿ç”¨å¤§å†™å­—æ¯å’Œæ•°å­—ï¼Œå› ä¸ºè¿™äº›å’Œä¸­æ–‡æ··æ’æœ€å¸¸è§
    (ä¾‹å¦‚ "JSON æ–‡ä»¶"ã€"100% å®Œæˆ")
    """
    codepoints = []
    # å¤§å†™å­—æ¯ A-Z
    codepoints.extend(range(ord('A'), ord('Z') + 1))
    # æ•°å­— 0-9
    codepoints.extend(range(ord('0'), ord('9') + 1))
    return codepoints


# ==================== ä¸»æµç¨‹ ====================


def measure_font(
    font_key: str,
    font_path: str,
    cache: dict,
    sample_codepoints: list[int] | None = None,
) -> dict | None:
    """æµ‹å®šå­—ä½“ metricsï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
    if not font_path or not os.path.exists(font_path):
        print(f"  âš ï¸ {font_key}: æ–‡ä»¶ä¸å­˜åœ¨ - {font_path}")
        return None

    # æ£€æŸ¥ç¼“å­˜
    if is_cache_valid(font_path, cache):
        print(f"  âœ“ {font_key}: ä½¿ç”¨ç¼“å­˜ - {os.path.basename(font_path)}")
        return cache[font_path]["metrics"]

    # é‡æ–°æµ‹å®š
    print(f"  â†’ {font_key}: æµ‹å®šä¸­ - {os.path.basename(font_path)}")

    metrics = get_font_metrics(font_path)
    if not metrics:
        return None

    # é‡‡æ ·å­—å½¢é«˜åº¦
    if sample_codepoints:
        glyph_stats = sample_glyph_heights(font_path, sample_codepoints)
        if glyph_stats:
            metrics["glyph_stats"] = glyph_stats

    # æ›´æ–°ç¼“å­˜
    cache[font_path] = {
        "mtime": os.path.getmtime(font_path),
        "hash": get_file_hash(font_path),
        "metrics": metrics,
    }

    return metrics


def calculate_offsets(english_metrics: dict, chinese_metrics: dict, icon_metrics: dict) -> dict:
    """è®¡ç®—åˆå¹¶å­—ä½“éœ€è¦çš„ offset

    åŸºäºå­—å½¢è§†è§‰ä¸­å¿ƒè®¡ç®—ã€‚

    åŸç†ï¼š
    - glyph_offset_y æ­£å€¼ = å­—å½¢å‘ä¸‹ç§»åŠ¨
    - å¦‚æœå­—å½¢è§†è§‰ä¸Šåä¸Šï¼Œéœ€è¦æ­£å€¼å‘ä¸‹ç§»åŠ¨

    å­—å½¢ä¸­å¿ƒ (avg_center) æ˜¯ç›¸å¯¹äº baseline çš„ Y åæ ‡ï¼ˆå­—ä½“åæ ‡ç³»ï¼Œå‘ä¸Šä¸ºæ­£ï¼‰
    ä½† ImGui æ¸²æŸ“æ—¶ Y è½´å‘ä¸‹ï¼Œæ‰€ä»¥éœ€è¦åè½¬
    """
    result = {
        "chinese_offset_y": 0.0,
        "icon_offset_y": 0.0,
    }

    # è·å–è‹±æ–‡å­—å½¢çš„è§†è§‰ä¸­å¿ƒï¼ˆå¤§å†™+æ•°å­—ï¼‰
    en_center = 0.0
    if "glyph_stats" in english_metrics:
        en_center = english_metrics["glyph_stats"].get("avg_center", 0)

    # ä¸­æ–‡ offset: è®©ä¸­æ–‡å­—å½¢ä¸­å¿ƒå¯¹é½è‹±æ–‡å­—å½¢ä¸­å¿ƒ
    # en_center > cn_center è¯´æ˜ä¸­æ–‡åœ¨å±å¹•ä¸Šåä¸Šï¼Œéœ€è¦æ­£å€¼å‘ä¸‹ç§»åŠ¨
    if chinese_metrics and "glyph_stats" in chinese_metrics:
        cn_center = chinese_metrics["glyph_stats"].get("avg_center", 0)
        result["chinese_offset_y"] = round(en_center - cn_center, 1)

    # å›¾æ ‡ offset
    if icon_metrics and "glyph_stats" in icon_metrics:
        icon_center = icon_metrics["glyph_stats"].get("avg_center", 0)
        scaled_icon_center = icon_center * ICON_SCALE
        result["icon_offset_y"] = round(en_center - scaled_icon_center, 1)

    return result


def generate_python_file(
    font_paths: dict[str, str],
    english_metrics: dict,
    chinese_metrics: dict | None,
    icon_metrics: dict | None,
    offsets: dict,
) -> str:
    """ç”Ÿæˆ Python é…ç½®æ–‡ä»¶å†…å®¹"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    lines = [
        "# -*- coding: utf-8 -*-",
        '"""å­—ä½“é…ç½® (è‡ªåŠ¨ç”Ÿæˆ)',
        "",
        "ç”± codegen/generate_font_config.py è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·å‹¿æ‰‹åŠ¨ç¼–è¾‘ã€‚",
        f"ç”Ÿæˆæ—¶é—´: {timestamp}",
        "",
        "ä¿®æ”¹å­—ä½“é…ç½®è¯·ç¼–è¾‘ codegen/generate_font_config.py ä¸­çš„ FONT_PATHS å’Œ ICON_SCALEã€‚",
        '"""',
        "",
        "from __future__ import annotations",
        "",
        "# ==================== å­—ä½“è·¯å¾„ ====================",
        "",
        f'ENGLISH_FONT = "{font_paths["english"]}"',
        f'CHINESE_FONT = "{font_paths["chinese"]}"',
        f'ICON_FONT = "{font_paths["icon"]}"',
        "",
        "# ==================== åŸºå‡†å­—å· ====================",
        "",
        f"BASE_FONT_SIZE = {BASE_FONT_SIZE}  # ä¸‹æ–¹æ‰€æœ‰åº¦é‡å€¼åŸºäºæ­¤å­—å·",
        "",
        "# ==================== ä¸»å­—ä½“ Metrics ====================",
        f"# {os.path.basename(font_paths['english'])}",
        "",
        f"ENGLISH_HHEA_ASCENT = {english_metrics.get('hhea_ascent', 0)}",
        f"ENGLISH_HHEA_DESCENT = {english_metrics.get('hhea_descent', 0)}",
        "",
        "# ==================== Baseline åç§»è¡¥å¿ ====================",
        "# ImGui åˆå¹¶å­—ä½“æ—¶ï¼Œåç»­å­—ä½“æ²¿ç”¨ä¸»å­—ä½“çš„ metrics",
        "# å¦‚æœ ascent ä¸åŒï¼Œä¼šäº§ç”Ÿå‚ç›´åç§»ï¼Œéœ€è¦ç”¨ glyph_offset è¡¥å¿",
        "#",
        "# å…¬å¼: actual_offset = BASE_OFFSET * (font_size / BASE_FONT_SIZE)",
        "",
    ]

    # ä¸­æ–‡åç§»
    if chinese_metrics:
        cn_name = os.path.basename(font_paths["chinese"])
        lines.extend([
            f"# ä¸­æ–‡å­—ä½“: {cn_name}",
            f"# hhea_ascent = {chinese_metrics.get('hhea_ascent', 0)} (ä¸»å­—ä½“ = {english_metrics.get('hhea_ascent', 0)})",
            f"CHINESE_GLYPH_OFFSET_Y = {offsets['chinese_offset_y']}",
            "",
        ])
    else:
        lines.extend([
            "# ä¸­æ–‡å­—ä½“: æœªé…ç½®",
            "CHINESE_GLYPH_OFFSET_Y = 0.0",
            "",
        ])

    # å›¾æ ‡åç§»å’Œç¼©æ”¾
    if icon_metrics:
        icon_name = os.path.basename(font_paths["icon"])
        scaled_ascent = icon_metrics.get('hhea_ascent', 0) * ICON_SCALE
        lines.extend([
            f"# å›¾æ ‡å­—ä½“: {icon_name}",
            f"# hhea_ascent = {icon_metrics.get('hhea_ascent', 0)} Ã— ICON_SCALE = {scaled_ascent:.2f} (ä¸»å­—ä½“ = {english_metrics.get('hhea_ascent', 0)})",
            f"ICON_GLYPH_OFFSET_Y = {offsets['icon_offset_y']}",
            "",
            "# å›¾æ ‡ç¼©æ”¾ (è®¾è®¡å‚æ•°ï¼Œä» codegen å¤åˆ¶)",
            f"ICON_SCALE = {ICON_SCALE}",
        ])
    else:
        lines.extend([
            "# å›¾æ ‡å­—ä½“: æœªé…ç½®",
            "ICON_GLYPH_OFFSET_Y = 0.0",
            "ICON_SCALE = 1.0",
        ])

    lines.extend([
        "",
        "# ==================== å›¾æ ‡ç ç‚¹èŒƒå›´ ====================",
        "",
        "ICON_RANGE_START = 0xE000",
        "ICON_RANGE_END = 0xF8FF",
    ])

    return "\n".join(lines) + "\n"


def main():
    print("=" * 60)
    print("ğŸ“ ç”Ÿæˆå­—ä½“é…ç½®")
    print("=" * 60)

    # æ£€æŸ¥å­—ä½“æ–‡ä»¶
    print("\nå­—ä½“è·¯å¾„:")
    all_exist = True
    for key, path in FONT_PATHS.items():
        exists = os.path.exists(path)
        status = "âœ“" if exists else "âœ—"
        print(f"  {status} {key}: {path}")
        if not exists:
            all_exist = False

    if not all_exist:
        print("\nâŒ éƒ¨åˆ†å­—ä½“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
        sys.exit(1)

    print(f"\nå›¾æ ‡ç¼©æ”¾: ICON_SCALE = {ICON_SCALE}")

    # åŠ è½½ç¼“å­˜
    cache = load_cache()
    print(f"\nç¼“å­˜æ¡ç›®: {len(cache)}")

    # æµ‹å®šå„å­—ä½“ (åŒ…æ‹¬å­—å½¢ä¸­å¿ƒé‡‡æ ·)
    print("\næµ‹å®š metrics:")

    english_metrics = measure_font(
        "english",
        FONT_PATHS["english"],
        cache,
        sample_codepoints=get_english_codepoints(),  # é‡‡æ ·è‹±æ–‡å­—å½¢ä¸­å¿ƒ
    )
    chinese_metrics = measure_font(
        "chinese",
        FONT_PATHS["chinese"],
        cache,
        sample_codepoints=get_gb2312_codepoints(),
    )
    icon_metrics = measure_font(
        "icon",
        FONT_PATHS["icon"],
        cache,
        sample_codepoints=get_icon_codepoints(),
    )

    # ä¿å­˜ç¼“å­˜
    save_cache(cache)

    # æ‰“å°è¯¦ç»†çš„å­—å½¢ç»Ÿè®¡
    print("\nå­—å½¢ç»Ÿè®¡:")
    for name, metrics in [("english", english_metrics), ("chinese", chinese_metrics), ("icon", icon_metrics)]:
        if metrics and "glyph_stats" in metrics:
            stats = metrics["glyph_stats"]
            print(f"  {name}:")
            print(f"    é‡‡æ ·æ•°: {stats['sample_count']}")
            print(f"    å¹³å‡é«˜åº¦: {stats['avg_height']}px")
            print(f"    å¹³å‡ä¸­å¿ƒ: {stats['avg_center']}px (ç›¸å¯¹baseline)")

    # è®¡ç®—åç§»
    offsets = calculate_offsets(english_metrics, chinese_metrics or {}, icon_metrics or {})

    print("\nè®¡ç®—ç»“æœ:")
    print(f"  ä¸­æ–‡ glyph_offset_y = {offsets['chinese_offset_y']}px")
    print(f"  å›¾æ ‡ glyph_offset_y = {offsets['icon_offset_y']}px (åŸºäº ICON_SCALE={ICON_SCALE})")

    # ç”Ÿæˆ Python æ–‡ä»¶
    content = generate_python_file(
        FONT_PATHS,
        english_metrics,
        chinese_metrics,
        icon_metrics,
        offsets,
    )

    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(content)

    print(f"\nâœ… å·²ç”Ÿæˆ: {OUTPUT_FILE}")
    print("=" * 60)


if __name__ == "__main__":
    main()
